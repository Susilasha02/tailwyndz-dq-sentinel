name: DQ Sentinel

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  run-dq:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set PYTHONPATH to repo root
        run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10.x'
          check-latest: false

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --prefer-binary -r requirements.txt || pip install -r requirements.txt
          pip install pytest

      - name: Prepare cleaned data (create data/cleaned and run cleaning scripts)
        run: |
          set -e
          # Ensure cleaned dir exists
          mkdir -p data/cleaned

          # Helper: run dedupe/TZ fixer if script exists, otherwise fallback to copying the original
          run_fix_or_copy() {
            local src="$1"
            local script="$2"
            local out="data/cleaned/$(basename "$src")"
            if [ -f "$script" ]; then
              echo "Running $script on $src -> $out"
              python "$script" "$src" || { echo "Script failed; copying original"; cp "$src" "$out"; }
            else
              echo "Script $script not found, copying $src -> $out"
              cp "$src" "$out"
            fi
          }

          # Run fixes for known problematic files (adjust names if your scripts have different names)
          if [ -f data/sales_weekly_dupes.csv ]; then
            run_fix_or_copy data/sales_weekly_dupes.csv scripts/fix_dup.py
          fi

          if [ -f data/sales_weekly_partial_backfill.csv ]; then
            run_fix_or_copy data/sales_weekly_partial_backfill.csv scripts/fix_dup.py
          fi

          if [ -f data/sales_weekly_schema_v2.csv ]; then
            run_fix_or_copy data/sales_weekly_schema_v2.csv scripts/fix_schema_v2.py
          fi

          # For the rest of the sales_weekly*.csv files, copy them to cleaned if not already created
          for f in data/sales_weekly*.csv; do
            base=$(basename "$f")
            if [ ! -f "data/cleaned/$base" ]; then
              echo "Copying $f -> data/cleaned/$base"
              cp "$f" "data/cleaned/$base"
            else
              echo "data/cleaned/$base already present (created by cleaner)"
            fi
          done

      - name: Run unit tests
        run: pytest -q

      - name: Run DQ sentinel (uses data/cleaned)
        run: |
          python src/dq_sentinel.py --data-dir data/cleaned --out-dir reports/ci --calendar data/calendar.csv --promos data/promos.csv --pattern "sales_weekly*.csv"

      - name: Upload reports artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dq-reports
          path: reports/ci
